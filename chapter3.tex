\chapter{METHODOLOGY}
{\baselineskip=2\baselineskip
	
This chapter presents the research methodology used in the study, it covers the in-depth details of the procedures and steps taken in the research design, design procedure, system development, and deployment process of this study.


\section { System Architecture}

\begin{figure}[h]
	\centering
	\includegraphics[height=0.5\textheight]{figures/Architectureofniggas}
	\caption{System Architecture}
	\label{fig:System Architecture}
\end{figure}

Figure 2 shows the system architecture of the pig weight estimation system. It consists of the pig to be captured, the Kinect camera that will be used for taking depth images, and the system for the processing of all the data. A laptop will be used for the processing of data, which will include the processing of images, extraction of pig’s features, and calculation of estimated weight with a machine learning model. The result can then be accessed in a web application, where actual and estimated weights can be validated.

\subsection { Waterfall Model}
\begin{figure}[h]
	\centering
	\includegraphics[height=0.3\textheight]{figures/Requirement Analysis (4)}
	\caption{Waterfall Model}
	\label{fig:Waterfall Model}
\end{figure}

Figure 3 shows the Waterfall Model of the Weight Estimation System. Requirement Analysis, defines the goal for creating a non-invasive pig weight estimation system using a Kinect depth sensor. The system captures depth images, extracts features like height and volume, and predicts weight through machine learning models, with results available via a web app. System Design plans the architecture, including hardware and software components. For the hardware components, a Kinect sensor, a computer system, and a pig pen setup are required. While a Machine learning model and a web application is required for the software components. Implementation involves collecting images, extracting features, and training machine learning models. The Kinect sensor gathers the images to be processed. An algorithm extracts the features such as the height, area and volume from the depth images. The image processing involves grayscale conversion of the gathered images, contour detection and feature segmentation to isolate pig features. The training of the machine learning model follows a ratio of 7:3 for the training data and the testing data respectively. The testing phase assesses the system's accuracy using performance metrics such as MAE and RMSE. Following the 7:3 ratio, 30 percent of the gathered data set is used in the testing of the machine learning models in which the most accurate model is selected. The system is deployed using a web application that displays the weight estimates based on input depths.
\newpage
\subsection {Flowchart}
\begin{figure}[h]
	\centering
	\includegraphics[height=0.6\textheight]{figures/Thesis Flowchart}
	\caption{Flowchart}
	\label{fig:Flowchart}
\end{figure}

Figure 4 outlines the flowchart of the weight estimation system. The process begins with system initialization, where the Kinect camera and other equipment are powered on, and the pig is prepared for imaging. Images are captured using the Kinect camera, and a good quality image is defined as clear, well-lit, free of motion blur, and accurately representing the pig's dimensions. To select the highest quality image, a max voting ensemble algorithm evaluates multiple captured images based on metrics like sharpness, contrast, and brightness (Gonzalez and Woods, 2018). If no satisfactory image is found, additional captures are made until a quality image is obtained. Once a high-quality image is secured, the system moves on to image preprocessing and feature extraction, leading to the prediction of the estimated weight. The final result is then displayed in a web application for user access.
\subsection {Pigpen Diagram}

\begin{figure}[h]
	\centering
	\includegraphics[height=0.4\textheight]{figures/Untitled-1wqw}
	\caption{Pigpen Diagram}
	\label{fig:Pigpen Diagram}
\end{figure}

Figure 5 presents a diagram of the pigpen, illustrating the positioning of the Kinect camera. The camera is strategically suspended 1.25 meters above the ground. This height ensures a clear capture of the pig within the pen, while remaining low enough to minimize the likelihood of capturing workers as they move around the area. Additionally, this positioning keeps the camera within its optimal range of 2 meters, maximizing its effectiveness for monitoring purposes. This setup is designed to predict the weight of one pig at a time

\section{Study Design and Data Collection}
The study will be conducted in commercial pig weaning farms located in Northern Mindanao, Philippines, focusing on pigs of the Landrace breed. These pigs, with an average live weight ranging from 40 to 100 kilograms, are fed using commercial dry pellet feed and have access to water through standard feeders in the weaning pens. Data collection will take place within these farms, where depth images of the pigs will be captured using the Kinect V1 camera. The camera will record depth data, essential for calculating the distance between the sensor and the top of the pig, and between the sensor and the ground. This is needed for the calculation of the height of the pig, which will be used to estimate the pig's weight.
Several environmental factors will also be monitored during data collection, such as lighting conditions and temperature. The size of the pen in the weaning barn will be recorded to ensure consistent environmental conditions for all pigs observed. Additionally, the temperature of the area during observation will be measured, as fluctuations in temperature can influence the pigs’ behavior and posture. The lighting conditions within the observation area will also be documented, given that lighting can impact the quality of the depth images captured by the Kinect sensor. By controlling and documenting these variables, the study aims to minimize external factors that could affect the accuracy of the weight estimation model.

\section{Object Detection}
Object detection in this study isolates the pig within each frame using depth images from Kinect sensors to identify the region of interest (ROI). We employ YOLOv11 (You Only Look Once, version 11), a state-of-the-art single-shot object detection model that delivers superior efficiency and accuracy in real-time applications.

YOLOv11 incorporates several advanced features that make it ideal for agricultural applications, including pig detection. It utilizes an improved backbone and neck architecture, significantly enhancing its feature extraction capabilities. This enhancement allows the model to handle complex object detection tasks, such as identifying pigs with varying poses and sizes, with higher precision. Additionally, YOLOv11 introduces refined architectural designs and optimized training pipelines, achieving faster processing speeds without compromising accuracy, making it highly suitable for real-time detection in a dynamic farm environment.

The model achieves a higher mean Average Precision (mAP) on benchmark datasets, such as COCO, while using 22 percent fewer parameters compared to earlier versions like YOLOv8. This computational efficiency reduces hardware requirements, enabling deployment on edge devices without sacrificing performance. Its flexibility allows seamless integration across various platforms, including edge devices, cloud systems, and NVIDIA GPU-supported hardware, ensuring adaptability to different farm setups and computational infrastructures.

Furthermore, YOLOv11 supports a broad range of computer vision tasks, including instance segmentation, image classification, pose estimation, and oriented object detection. While this study focuses on object detection, YOLOv11’s versatility ensures scalability for future enhancements, making it a robust choice for agricultural applications.

A pre-trained YOLOv11 model, fine-tuned with depth image data specific to our study, was selected to leverage transfer learning. This approach minimizes training overhead and maximizes detection reliability. YOLOv11’s ability to process entire images in a single pass ensures optimized speed and accuracy—key factors for real-time agricultural applications. Its enhanced feature extraction and superior accuracy make it particularly well-suited for detecting complex and variable shapes, such as pigs, in the farm environment.


\section{Segmentation Model}
This study employs the Segmentation Anything Model (SAM) to achieve precise segmentation of the pig within the detected ROI. SAM is uniquely suited for this task due to its prompt-based, generalizable approach, which allows for accurate segmentation with minimal retraining (Kirillov et al., 2023). SAM’s adaptability is essential in agricultural settings, where it reliably captures complex shapes and contours, ensuring high-quality feature extraction of key metrics like height and area. By using SAM, we enhance the segmentation accuracy and efficiency, enabling more consistent and precise data for weight estimation.
\subsection{Image Preprocessing}
During the preprocessing stage, the image is converted to grayscale and resized. If its dimensions are smaller than the target size, symmetric padding is applied to ensure the entire pig is visible within the frame. Next, a Gaussian blur with a kernel size of (13,13) is applied; this kernel size is chosen arbitrarily to balance blurring and detail preservation. The blurred image is then subtracted from the original to enhance edges, expressed as follows:


\begin{equation}
	I_{\text{sharp}}(x, y) = I(x, y) + \alpha (I(x, y) - I_{\text{blur}}(x, y))
\end{equation}
\myequation{Image Processing}

where I(x,y) is the original intensity and Iblur(x,y) is the blurred intensity. A threshold value of T=5 is chosen for binarization to effectively distinguish the pig’s silhouette from the background; this value is determined based on preliminary testing to optimize contrast without losing important details. This preprocessing workflow ensures a standardized input for subsequent models, facilitating accurate feature extraction.

\subsection{Slicing the Image}
The image is split into upper and lower halves based on the principal direction of the pig’s body. This is achieved using the eigenvalue decomposition of the covariance matrix derived from the contour points of the pig. First, the centroid of the contour is calculated using the image moments:

\begin{equation}
	cX = \frac{M_{10}}{M_{00}}, \quad cY = \frac{M_{01}}{M_{00}}
\end{equation}
\myequation{Centroid of the Contour}

Where $M_00$ is the area of the contour, and$M_10$, $M_01$ are the first-order moments along with the x and y axes, respectively.
The covariance matrix $\sigma$ of the contour points is then computed as:

\begin{equation}
	\Sigma = \frac{1}{N} \sum_{i=1}^{N} (p_i - \mu)(p_i - \mu)^T
\end{equation}
\myequation{Covariance Matrix}

Where $p_1$ represents a contour point, $\mu$ is the mean of the contour points, and N is the total number of points. The eigenvector corresponding to the largest eigenvalue defines the primary direction along which the image is sliced. The eigenvector, $v=[v_x, v_y]^T$ , is used to define the slope of the line that divides the pig into upper and lower halves:

\begin{equation}
	y_line(x)=\frac{v_y}{v_x} (x-cX) + cY
\end{equation}
\myequation{Slicing the Image}

After slicing, gaps in the binary mask are filled using a morphological closing operation. This ensures that the segmentation is continuous, filling any small holes that might have been created during slicing.

\begin{equation}
	I_{\text{filled}} = I_{\text{mask}} \oplus K
\end{equation}
\myequation{Morphological Closing}

where K is a kernel of size (5,5) used for morphological closing.

\subsection{Valley Detection}
To segment the pig into distinct anatomical regions (head, body, legs), we detect valleys along the contour. Valleys correspond to local minima in the pig’s contour, typically where the legs meet the body or the neck separates the head from the shoulders.
First, we extract the outline of the pig and smooth the contour using linear interpolation. The interpolated contour points are then analyzed using peak detection on the inverted y-coordinates to find the valleys, representing the points of maximum curvature.
let ${(x_i, y_i)}^N _i=1$ represent the sorted interpolated contour points. The valleys are found by identifying the local maxima in the inverted contour:

\begin{equation}
	v_i=arg max(-y(x))
\end{equation}
\myequation{Valley Detection}

The detected valleys are sorted based on their xxx-coordinates to establish the segmentation boundaries.

\subsection{Segmentation into Sections}
Using the detected valleys, we define four primary sections: legs, body, shoulders, head. Each section is extracted by crearing binary masks between consecutive valleys:

\begin{equation}
	S_region (x,y) = I_sharp (x,y) (1_[x_start,x_end])
\end{equation}
\myequation{Segmented Region of Interest}

where $S_region$ is the segmented region of interest.

\begin{figure}[h]
	\centering
	\includegraphics[height=0.4\textheight]{figures/Untitled-1 (1)}
	\caption{Schematic representation of segmentation}
	\label{fig: Schematic representation of segmentation}
\end{figure}

Figure 5 shows a schematic representation of the segmentation of the pig. Following the segmentation process found in the study conducted by, the pig is segmented into four distinct parts: the hind legs, body, shoulder, and head. Using this segmentation framework, an algorithm will be developed to accurately divide the pig into these four areas. This is due to the difference in the pigs overall shape, allowing for more detailed data analysis and improved precision in data collection.

\section{Feature Extraction}
After segmentation, depth information is used to extract key features from the pig’s body. The Kinect sensor provides depth data $D(x,y)$, where the $x-axis$ and $y-axis$ represent the horizontal and vertical pixel coordinates of each point on the pig’s body, respectively. This data indicates the distance between the sensor and a specific point on the pig at a given pixel location $(x,y)$. Important features derived from this data include height, age, area, and gender.

\subsection{Height Extraction}
The height of the pig is determined by finding the maximum and minimum depth values within the segmented region. Let S(x,y) be the segmented mask of the pig. The height is calculated as:

\begin{equation}
	x_{\text{highest}} = \max\{D(x, y) \mid (x, y) \in S(x, y)\}
\end{equation}
\myequation{Highest Intensity}

\begin{equation}
	x_{\text{lowest}} = \min\{D(x, y) \mid (x, y) \in S(x, y)\}
\end{equation}
\myequation{Lowest Intensity}

\begin{equation}
	y_{\text{height}} = x_{\text{highest}} - x_{\text{lowest}}
\end{equation}
\myequation{Height Extraction}

where $x_highest$ denotes the highest intensity and $x_lowest$ denotes the lowest intensity $S(x,y)$.

\subsection{Area Extraction}
The area occupied by the pig in the image is calculated by counting the number of pixels in the segmented region:

\begin{equation}
	Area= \sum_{(x, y) \in S(x, y)} 1
\end{equation}
\myequation{Area Extraction}

The area corresponds to the total number of pixels within the segmented pig’s contour, representing the surface area in the depth image.

\subsection{Gradient Information from Depth Data}
The depth gradient across the pig’s body provides additional insight into the pig’s shape. The gradient of the depth image D(x,y) can be computed using the finite difference approximation:

\begin{equation}
	\delta(x,y)= (\frac{\partial}{\partial x} , \frac{\partial}{\partial y})
\end{equation}
\myequation{Depth Gradient}

This gradient gives us information about the slope of the pig’s body, which can be used to under more detailed features such as the body curvature and the slope of different anatomical regions. The gradient magnitude can be used to detect regions of rapid change in depth, such as the transition from the pig’s back to its sides:

\begin{equation}
	\left| \delta(x,y) \right| = \sqrt{\left( \frac{\partial}{\partial x} \right)^2 + \left( \frac{\partial}{\partial y} \right)^2}
\end{equation}
\myequation{Gradient Magnitude}

\section{Model Training}
In the model training phase, the extracted features will serve as inputs to various regression models aimed at estimating pig weight. We will explore several algorithms to identify the most accurate model for weight estimation: Linear Regression, Ridge Regression, Decision Trees, LightGBM, and XGBoost.

The analysis begins with linear regression, which models the linear relationship between features and weight, providing a straightforward baseline for comparison with more complex models (Mullainathan and Spiess, 2017). Ridge regression will be employed as an extension of linear regression, incorporating L2 regularization to mitigate overfitting, especially in cases of multicollinearity among input features. This approach enhances robustness against noisy data (Tikhonov, 1963).

Decision trees will also be utilized for their interpretability and ability to capture complex relationships within the data, though they require careful tuning to avoid overfitting (Breiman et al., 1986). LightGBM (Light Gradient Boosting Machine) is included for its efficiency and scalability with large datasets, utilizing a histogram-based learning approach to achieve faster training and improved accuracy (Ke et al., 2017). Additionally, XGBoost (Extreme Gradient Boosting) will be assessed for its high performance and optimizations for handling missing values and regularization, making it a leading choice in competitive machine-learning environments (Chen and Guestrin, 2016).
Model performance will be evaluated using metrics such as Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). MAE provides a clear measure of average error, while RMSE emphasizes larger errors, facilitating a comprehensive assessment of the model's accuracy (Willmott and Matsuura, 2005).

\section{Evaluation}
The trained models will undergo rigorous evaluation using a dedicated test dataset, comprising previously unseen data sampled randomly, which constitutes 30 percent of the overall dataset. This approach ensures that the evaluation accurately reflects the model's ability to generalize to new instances rather than merely memorizing the training data.
\subsection{Test Dataset}
By reserving 30 percent of the total dataset as the test dataset, we establish a method for assessing model performance. This test dataset will not have been exposed to the models during training, which is crucial for evaluating how well the models can predict weights on new data. The random sampling method ensures a representative distribution of weights and features, allowing for a fair assessment of the model's predictive capabilities.

\subsection{Performance Metrics}

To quantitatively evaluate the models, we will utilize the following performance metrics:
Mean Absolute Error (MAE). MAE calculates the average absolute difference between the actual values and the predicted values. It is a robust metric that does not penalize larger errors as heavily as MSE, making it more appropriate for interpreting the accuracy of regression models. The formula for MAE is:

\begin{equation}
	MAE = \frac{1}{N} \sum_{t=1}^{N} \left| \hat{y_t} - y_t \right|
\end{equation}
\myequation{Mean Absolute Error}

The MAE metric gives a straightforward interpretation of error, as it expresses the average error directly in terms of the target variable (i.e., weight). It also equally weights all errors, making it a preferred choice for evaluating models that must perform reliably across a range of errors.

\subsection{Root Mean Squared Error (RMSE)}
RMSE provides a measure of how well the model's predictions match the actual data, expressed in the same units as the target variable. It is particularly useful as it penalizes larger errors more heavily, making it sensitive to outliers. The equation for RMSE is given by:

\begin{equation}
	RMSE = \sqrt{\frac{1}{N} \sum_{t=1}^{N} \left( y_t - \hat{y_t} \right)^2}
\end{equation}
\myequation{Root Mean Squared Error}

Where n is the number of observations in the test dataset, yi is the actual weight of the pig for the t-th observation, and yi is the predicted weight of the pig for the t-th observation.

\section{Testing Method}
The trained model will undergo rigorous evaluation to validate its accuracy. This evaluation will involve a cross-validation process utilizing a new set of pigs in a real-world experimental setup. The experimental process includes testing the system by comparing the predicted weights to the actual weights obtained using a traditional weighing scale. This approach ensures that the model is tested under practical farming conditions and its results are benchmarked against ground truth values.

\section{Location}
The experimental testing will take place at a backyard pig-weaning farm in El Salvador City, Misamis Oriental, Philippines. This location was selected due to its accessibility and suitability for the study. The study will focus on Landrace pigs, a breed well-suited for controlled weight estimation studies, with an average weight range of 40–100 kg. Environmental variables such as lighting, temperature, and pen dimensions will be monitored to ensure consistent conditions throughout the testing period.

\section{Data Collection}
The Kinect sensor will be mounted in a fixed position above the pigpen to capture real-time depth images of the pigs. The system will continuously monitor the pigs and process images in real time to estimate their weight. Each depth image captured by the Kinect sensor will undergo preprocessing to ensure it meets the quality standards required for accurate feature extraction. Low-quality images, such as those affected by motion blur or poor lighting, will be automatically filtered out by an image quality check embedded in the system.

Once an acceptable image is identified, the system will extract features such as body height, surface area, and volume directly from the depth data. These features will then be inputted into a pre-trained machine-learning model to predict the pig's weight instantly. The estimated weight will be displayed in real time on a connected interface, such as a web application, for immediate validation and monitoring.

The Kinect sensor will be calibrated at the start of the observation period. This calibration will account for environmental factors such as lighting conditions and the sensor's positioning relative to the ground. For validation purposes, the predicted weight from the system will be periodically compared to the reference weight measured using a traditional weighing scale. These comparisons will be conducted during specific intervals to ensure the accuracy and consistency of real-time predictions.

\section{Experimental Setup}

The experiment aims to evaluate the system's accuracy in real-time pig weight prediction under controlled conditions. The setup includes a Kinect sensor, a computing unit, and a software pipeline for processing depth data and predicting pig weights. The details of the experimental setup are as follows:

\subsection{Environment Preparation}

\begin{description}
	
	\item[Pigpen Setup.]
	The experiment will take place in a typical pigpen environment to simulate real-world conditions. The area will be cleared of obstructions to ensure the Kinect sensor has an unobstructed view of the pigs.
	
	\item[Lighting Conditions.]
	Ambient lighting will be adjusted to minimize shadows and reflections. While the Kinect sensor primarily uses depth imaging, consistent lighting supports system calibration and overall reliability.
	
	\item[Sensor Placement.]
	The Kinect sensor will be mounted overhead, approximately 1.25 meters above the ground, ensuring a clear top-down view of the pigs. The mounting structure will be stable to prevent movement during the experiment.
	
\end{description}

\subsection{Kinect Sensor Calibration}

\begin{description}
	
	\item[Height and Angle.]
	The sensor will be carefully positioned, 1.25 meters from the ground, to ensure its depth camera captures accurate data, with the angle adjusted to be perpendicular to the ground.
	
	\item[Validation with Calibration Objects.]
	Objects of known dimensions will be placed within the pigpen to verify the sensor’s accuracy in capturing depth and spatial measurements.
	
\end{description}

\subsection{Data Collection and Processing}

\begin{description}
	\item[Image Capture.]
	The Kinect sensor will continuously capture depth images of pigs moving naturally within the pen. These images will be processed in real time to extract relevant features.
	
	\item[Feature Extraction.]
	Using depth images, the system will extract features critical to weight prediction, including:
	\subitem 
	\textbf{Body Height:} The vertical measurement of the pig from the ground.
	\subitem
	\textbf{Surface Area:} The top-down projected area of the pig.
	\subitem
	\textbf{Volume:} Estimated using depth data to represent the pig’s three-dimensional structure.
	
	\item[Weight Prediction.]
	Extracted features will be fed into a pre-trained machine learning model that predicts the pig's weight. The model is based on historical data correlating these features with known weights.
	
\end{description}

\subsection{Accuracy Evaluation}

\begin{description}
	\item[Reference Weights.]
	A subset of pigs will be manually weighed using a calibrated digital scale to provide ground truth measurements. These reference weights will be used to assess the accuracy of the system's predictions.
	
	\item[Performance Metrics.]
	The predicted weights will be compared against the reference weights using the following statistical measures:
	\subitem
	\textbf{Mean Absolute Error (MAE):} Indicates the average absolute deviation of predictions from the true values.
	\subitem
	\textbf{Root Mean Square Error (RMSE):} Highlights the magnitude of prediction errors, emphasizing larger discrepancies.
	\subitem
	\textbf{Correlation Coefficient (R²):} Evaluates the strength and reliability of the relationship between predicted and actual weights.
	
	\item[Repeated Measurements.]
	To ensure consistency, weight predictions will be recorded multiple times for each pig under varying activity levels (e.g., standing, lying down) and averaged to minimize variability.
	
\end{description}

\subsection{Validation Process}
The experiment will be conducted in several sessions, with data collected from different pigs of varying sizes to evaluate the system’s ability to generalize. Results will be analyzed to identify any systematic errors, such as over- or under-prediction for specific weight ranges, and adjustments will be made to the machine learning model if necessary.